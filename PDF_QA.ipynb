{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "# Initialize the LLM\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "folder_path = \"document\"  # Replace with the path to your folder containing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_txt_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all TXT files from a specified folder and return them as a list of LangChain Documents.\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing the TXT files.\n",
    "    Returns:\n",
    "        List[Document]: List of LangChain Documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            # Read text content from the file\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text_content = file.read()\n",
    "            # Create a LangChain Document for each file\n",
    "            doc = Document(page_content=text_content,\n",
    "                           metadata={\"source\": file_name})\n",
    "            documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Implement Small2Big Chunking ---\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "def small2big_chunking(documents, min_chunk_size=200, max_chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Apply the Small2Big chunking strategy to a list of documents and provide statistics.\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): List of documents to chunk.\n",
    "        min_chunk_size (int): Minimum chunk size.\n",
    "        max_chunk_size (int): Maximum chunk size.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of chunked documents.\n",
    "    \"\"\"\n",
    "    chunked_docs = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=min_chunk_size, chunk_overlap=50)\n",
    "\n",
    "    total_original_length = 0\n",
    "    total_chunked_length = 0\n",
    "\n",
    "    for doc in documents:\n",
    "        # Keep track of the original length of the document\n",
    "        total_original_length += len(doc.page_content)\n",
    "\n",
    "        # Split into small chunks first\n",
    "        small_chunks = text_splitter.split_text(doc.page_content)\n",
    "        combined_chunk = \"\"\n",
    "\n",
    "        for chunk in small_chunks:\n",
    "            if len(combined_chunk) + len(chunk) <= max_chunk_size:\n",
    "                combined_chunk += \" \" + chunk\n",
    "            else:\n",
    "                chunked_docs.append(\n",
    "                    Document(page_content=combined_chunk.strip(), metadata=doc.metadata))\n",
    "                combined_chunk = chunk\n",
    "\n",
    "        # Add the last chunk if it's not empty\n",
    "        if combined_chunk:\n",
    "            chunked_docs.append(\n",
    "                Document(page_content=combined_chunk.strip(), metadata=doc.metadata))\n",
    "\n",
    "    # Calculate statistics\n",
    "    total_chunked_length = sum(len(doc.page_content) for doc in chunked_docs)\n",
    "    avg_chunk_size = total_chunked_length / \\\n",
    "        len(chunked_docs) if chunked_docs else 0\n",
    "\n",
    "    print(\"=== Small2Big Chunking Statistics ===\")\n",
    "    print(f\"Total Original Documents: {len(documents)}\")\n",
    "    print(f\"Total Original Length: {total_original_length} characters\")\n",
    "    print(f\"Total Chunks Created: {len(chunked_docs)}\")\n",
    "    print(f\"Total Chunked Length: {total_chunked_length} characters\")\n",
    "    print(f\"Average Chunk Size: {avg_chunk_size:.2f} characters\")\n",
    "    print(f\"Minimum Chunk Size: {min_chunk_size} characters\")\n",
    "    print(f\"Maximum Chunk Size: {max_chunk_size} characters\")\n",
    "\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Multi-Query Expansion ---\n",
    "from langchain.output_parsers import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class ParaphrasedQuery(BaseModel):\n",
    "    \"\"\"You have performed query expansion to generate a paraphrasing of a question.\"\"\"\n",
    "    paraphrased_query: str = Field(\n",
    "        ...,\n",
    "        description=\"A unique paraphrasing of the original question.\"\n",
    "    )\n",
    "\n",
    "\n",
    "system = \"\"\"\n",
    "You are an expert in query paraphrasing and expansion. \n",
    "\n",
    "Your task is to generate multiple different phrasings of the same user query. \n",
    "Ensure that each paraphrased query captures the original meaning while using different wording.\n",
    "\n",
    "If there are multiple common ways to phrase the question, or common synonyms for key terms, ensure all are included.\n",
    "\n",
    "You **must** return at least 3 distinct paraphrased versions of the input query.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([ParaphrasedQuery])\n",
    "query_analyzer = prompt | llm_with_tools | PydanticToolsParser(tools=[\n",
    "                                                               ParaphrasedQuery])\n",
    "\n",
    "\n",
    "def expand_query(query):\n",
    "    \"\"\"\n",
    "    Generate multiple semantically similar queries using an LLM.\n",
    "    Args:\n",
    "        query (str): Original query.\n",
    "    Returns:\n",
    "        List[str]: List of expanded queries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = query_analyzer.invoke({\"question\": query})\n",
    "        # Extract paraphrased queries\n",
    "        expanded_queries = [\n",
    "            q.paraphrased_query for q in response if isinstance(q, ParaphrasedQuery)]\n",
    "\n",
    "        if not expanded_queries or len(expanded_queries) < 3:\n",
    "            print(\n",
    "                \"Warning: Less than 3 paraphrased queries returned. Consider adjusting the prompt.\")\n",
    "\n",
    "        print(f\"Expanded Queries: {expanded_queries}\\n\")\n",
    "        return expanded_queries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query expansion: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Combine Results from Multi-Query Retrieval ---\n",
    "def multi_query_retrieval(expanded_queries, retriever, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform retrieval using multiple queries and combine results.\n",
    "    \"\"\"\n",
    "    combined_results = []\n",
    "    for query in expanded_queries:\n",
    "        results = retriever.get_relevant_documents(query)[:top_k]\n",
    "        combined_results.extend(results)\n",
    "\n",
    "    # Deduplicate results based on document content\n",
    "    unique_docs = {doc.page_content: doc for doc in combined_results}\n",
    "    return list(unique_docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def rank_with_cove(query, retrieved_docs, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Rank retrieved documents based on relevance using embeddings.\n",
    "    Args:\n",
    "        query (str): The user's input query.\n",
    "        retrieved_docs (List[Document]): List of retrieved documents.\n",
    "        embeddings: The embedding model used for similarity calculations.\n",
    "        top_k (int): The number of top documents to return.\n",
    "    Returns:\n",
    "        List[Document]: The top-ranked documents.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate embedding for the query\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    # Step 2: Compute embeddings for the retrieved documents\n",
    "    doc_contents = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "    doc_embeddings = embeddings.embed_documents(doc_contents)\n",
    "\n",
    "    # Step 3: Calculate cosine similarity between query and document embeddings\n",
    "\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "\n",
    "    # Step 4: Rank documents by similarity score\n",
    "\n",
    "    ranked_indices = sorted(range(len(similarities)),\n",
    "                            key=lambda i: similarities[i], reverse=True)\n",
    "\n",
    "    # Step 5: Select top-k documents\n",
    "\n",
    "    ranked_docs = [retrieved_docs[i] for i in ranked_indices[:top_k]]\n",
    "\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_rag_chain(query, retriever, llm, embeddings, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform retrieval-augmented generation with query optimization.\n",
    "    Args:\n",
    "        query (str): The user's input query.\n",
    "        retriever: The retrieval mechanism.\n",
    "        llm: The language model.\n",
    "        embeddings: The embedding model for ranking documents.\n",
    "        top_k (int): The number of top documents to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the generated response and retrieved documents.\n",
    "    \"\"\"\n",
    "    # Step 1: Expand the query using the LLM\n",
    "    expanded_queries = expand_query(query)\n",
    "\n",
    "    # Step 2: Retrieve documents using expanded queries\n",
    "    retrieved_docs = multi_query_retrieval(expanded_queries, retriever, top_k)\n",
    "\n",
    "    # Step 3: Rank documents using CoVe\n",
    "    ranked_docs = rank_with_cove(query, retrieved_docs, embeddings, top_k)\n",
    "\n",
    "    # Step 4: Combine the retrieved documents into a single context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in ranked_docs])\n",
    "\n",
    "    # Step 5: Define the system prompt with the context\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know.\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Question: {query}\"\n",
    "    )\n",
    "\n",
    "    # Step 6: Format the final prompt for the LLM\n",
    "    final_prompt = system_prompt.format(query=query)\n",
    "    # print(final_prompt)\n",
    "    # Step 7: Get the answer from the LLM\n",
    "    response = llm(final_prompt)  # Assuming llm takes input as a dictionary\n",
    "\n",
    "    return {\"query\": query, \"response\": response, \"retrieved_docs\": ranked_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents_from_txt_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Small2Big Chunking Statistics ===\n",
      "Total Original Documents: 1\n",
      "Total Original Length: 6005 characters\n",
      "Total Chunks Created: 7\n",
      "Total Chunked Length: 6517 characters\n",
      "Average Chunk Size: 931.00 characters\n",
      "Minimum Chunk Size: 200 characters\n",
      "Maximum Chunk Size: 1000 characters\n"
     ]
    }
   ],
   "source": [
    "# Apply Small2Big chunking\n",
    "chunked_documents = small2big_chunking(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrievers\n",
    "bm25_retriever = BM25Retriever.from_documents(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=chunked_documents, embedding=OpenAIEmbeddings()\n",
    ")\n",
    "vector_retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Combine retrievers using EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Less than 3 paraphrased queries returned. Consider adjusting the prompt.\n",
      "Expanded Queries: ['What does Usage Data information consist of?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of queries\n",
    "queries = [\n",
    "    \"Thông tin Usage Data bao gồm những gì?\",\n",
    "]\n",
    "\n",
    "# Initialize LLM and embeddings\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize a list to store results\n",
    "results_list = []\n",
    "\n",
    "# Loop through each query, perform RAG, and store the results\n",
    "for query in queries:\n",
    "    result = optimized_rag_chain(\n",
    "        query=query,\n",
    "        retriever=ensemble_retriever,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        top_k=5,\n",
    "    )\n",
    "    # Append the result to the list\n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query ===\n",
      "Thông tin Usage Data bao gồm những gì?\n",
      "================================== Ai Message ==================================\n",
      "\n",
      "Thông tin Usage Data bao gồm các dữ liệu như địa chỉ IP của máy tính, loại trình duyệt, phiên bản trình duyệt, các trang của dịch vụ mà bạn truy cập, thời gian và ngày tháng của chuyến thăm, thời gian bạn dành cho những trang đó, các định danh thiết bị duy nhất và các dữ liệu chẩn đoán khác.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "for result in results_list:\n",
    "    print(\"=== Query ===\")\n",
    "    print(result[\"query\"])\n",
    "    print(result[\"response\"].pretty_repr())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document 1 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: We may also collect information that your browser sends whenever you visit our Service or when you access the Service by or through a mobile device (\"Usage Data\"). This Usage Data may include (\"Usage Data\"). This Usage Data may include information such as your computer's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that you visit, version, the pages of our Service that you visit, the time and date of your visit, the time spent on those pages, unique device identifiers, and other diagnostic data. USE OF DATA Presight uses the collected data for various purposes:\n",
      "- To provide and maintain our Service\n",
      "- To notify you about changes to our Service - To notify you about changes to our Service\n",
      "- To allow you to participate in interactive features of our Service when you choose to do so\n",
      "- To provide customer support\n",
      "\n",
      "--- Document 2 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: By Role\n",
      "By Team\n",
      "\n",
      "PRIVACY POLICY\n",
      "\n",
      "\n",
      "LAST UPDATED 15 SEP 2023 At Presight, we are committed to protecting the privacy of our customers and visitors to our website. This Privacy Policy explains how we collect, use, and disclose information about our customers use, and disclose information about our customers and visitors. INFORMATION COLLECTION AND USE\n",
      "\n",
      "We collect several different types of information for various purposes to provide and improve our Service to you.\n",
      "\n",
      "TYPES OF DATA COLLECTED While using our Service, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you (\"Personal Data\"). Personally identifiable you (\"Personal Data\"). Personally identifiable information may include, but is not limited to: - Email address\n",
      "- First name and last name\n",
      "- Phone number\n",
      "- Address, State, Province, ZIP/Postal code, City\n",
      "- Cookies and Usage Data\n",
      "\n",
      "--- Document 3 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: help maintain data integrity and accuracy. You are encouraged to provide complete and valid information to ensure the smooth processing of their personal data. DISCLOSURE OF INFORMATION We may disclose your application data to third-party service providers who help us provide our services such as Datadog, AWS, Google Cloud and Google Workspace. We may also disclose your information Workspace. We may also disclose your information in response to a legal request, such as a subpoena or court order, or to protect our rights or the rights of others. SHARING OF PERSONAL DATA\n",
      "\n",
      "Your personal data will not be subject to sharing, transfer, rental or exchange for the benefit of third parties, including AI models. GOOGLE USER DATA AND GOOGLE WORKSPACE APIS In all cases when users authenticate the platform to Google Workspace, the following applies: - We do not retain or use Google User Data to develop, improve, or train generalized/non-personalized AI and/or ML models.\n",
      "\n",
      "--- Document 4 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: - We do not use Google Workspace APIs to develop, improve, or train generalized/non-personalized AI and/or ML models. - We do not transfer Google User Data to third-party AI tools for the purpose of developing, improving, or training generalized or non-personalized AI and/or ML models. DATA SECURITY - All data is encrypted both in transit and at rest, using industry-standard encryption methods. - We regularly perform security audits and vulnerability assessments to ensure the safety of our platform and the data stored within it. - Our employees are trained on best practices for data security, and access to customer data is restricted on a need-to-know basis. DATA RETENTION & DISPOSAL Customer data is retained for as long as the account is in active status. Data enters an “expired” state when the account is voluntarily closed. Expired account data will be retained for 60 days. account data will be retained for 60 days. After this period, the account and related data will be removed.\n",
      "\n",
      "--- Document 5 ---\n",
      "Metadata: {'source': 'privacy_policy.txt'}\n",
      "Content: QUALITY, INCLUDING DATA SUBJECTS' RESPONSIBILITIES FOR QUALITY - We are committed to maintaining the quality and accuracy of the personal information we collect and process.\n",
      "- We rely on data subjects to provide accurate and up-to-date information. - Data subjects have the responsibility to inform us of any changes or inaccuracies in their personal data. - If you believe that any information we hold about you is inaccurate, incomplete, or outdated, please contact us promptly to rectify the information. MONITORING AND ENFORCEMENT - We regularly monitor its data processing activities to ensure compliance with this privacy policy and applicable data protection laws. - In the event of a data breach or any unauthorized access to your personal information, we will notify you and the appropriate authorities as required by law. - We committed to cooperating with data protection authorities and complying with their advice and decisions regarding data protection and privacy matters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results_list:\n",
    "    # Loop through the retrieved documents for this query\n",
    "    for i, doc in enumerate(result[\"retrieved_docs\"], start=1):\n",
    "        if isinstance(doc, Document):\n",
    "            print(f\"--- Document {i} ---\")\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "            print(f\"Content: {doc.page_content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
